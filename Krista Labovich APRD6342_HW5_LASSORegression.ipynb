{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Krista Labovich APRD6342_HW5_LASSORegression.ipynb","provenance":[{"file_id":"19JfDR4vEyNqaqvS9XVA8ouaO2dgItSap","timestamp":1649958362558}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","source":["# Using LASSO Regression for Modeling Fat Sales Data"],"metadata":{"id":"d9H6ffDluW7k"}},{"cell_type":"markdown","metadata":{"id":"ougNAXnmeHG0"},"source":["## Make a copy!\n","\n","Before getting started, make a copy of this notebook into your own Google Drive:\n","\n","    Go to: File > Save a copy in Drive"]},{"cell_type":"markdown","source":["## ⚠️ Autograder ignore sections\n","\n","This notebook may contain one or more code cells wrapped with the following comment-tags:\n","\n","\n","The start tag:\n","\n","```\n","    #~~ grader-ignore:\n","```\n","\n","and the end tag:\n","\n","```\n","    #~~ /grader-ignore\n","```\n","\n","**These tags must be kept in place and not modified in order for the grader to work properly with your submission file.**"],"metadata":{"id":"w5mDGyxFpTMe"}},{"cell_type":"markdown","metadata":{"id":"u9IF4NXUVeua"},"source":["**Important note**\n","\n","There are no functions to implement. Instead, the grader will be looking at the values of a number of variables that should be assigned. Be sure to:\n","\n"," * follow the notebook instructions carefully\n"," * complete all parts that need to be completed\n"," * name things as instructed / don't change existing variable names\n","\n","\n","The grader will be looking at your values for all of the following:\n","\n"," * descriptors\n"," * top_predictors\n"," * train_error\n"," * test_error\n"," * rsquared_train\n"," * rsquared_test\n","\n","Sections 1 & 2 do not require additional code. Read the content of those sections and execute and understand the existing code.\n","\n","Section 3 has completion sections and requires that you write code to complete the assignment for submission to the grader."]},{"cell_type":"markdown","metadata":{"id":"lAf3Z0_v7vZ-"},"source":["## Section 1. Load and prepare the data\n","\n","In this first section of the assignment, you will not need to modify any of the code. Read through the section and execute the existing code. Be sure you understand what is happening.\n","\n","At the end of this section you will have the data loaded into training and testing data sets, ready for analsysis."]},{"cell_type":"markdown","metadata":{"id":"pMsJ11-z8D4h"},"source":["### Background"]},{"cell_type":"markdown","metadata":{"id":"wieV7nDgelWm"},"source":["Often in marketing analytics we have too much data to do a simple multiple regression. That is, there are too many possible predictors to consider at once. Multiple regression falls apart in these instances because of [multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity) and because often many variables will be significant, leaving us with no real idea what the few true factors driving the result we want truly are."]},{"cell_type":"markdown","metadata":{"id":"oQC3KH6cezVa"},"source":["Luckily, some newer regression-like approaches have emerged to help us handle this type of fat data (data with tons of possible predictors). This week you will work through a beautiful python implementation of [LASSO](https://en.wikipedia.org/wiki/Lasso_(statistics)) (least absolute shrinkage and selection operator). LASSO is great in that it preforms feature (predictor) variable selection. That is, it automatically selects the most powerful variables, the variables that explain the most variance in our regression, while leaving out those that explain little unique variance."]},{"cell_type":"markdown","metadata":{"id":"JiZve2Tle5ZV"},"source":["The dataset you're going to work on is sales data. \"# Purchases\" is the raw count of sales we saw for a calendar year in a particular are of the U.S. All of the other columns refer to Census data. That is, descriptives about the number of people living in a certain area. We'll be able to look up the census variables to figure out exactly what they mean later, but for now, I want you to focus on the idea that certain Census variables might predict sales. For instance, the number of households in an area that make over 100k a year in annual income might predict the amount of sales for luxury items, such as a Rolex. We're going to perform this analysis to see what Census variables most correlate to sales of our product, which was in this case was [Bobo Bars](https://eatbobos.com/)."]},{"cell_type":"markdown","metadata":{"id":"iibEOzoL56sk"},"source":["For this assignment, use the finalmaster-ratios.csv data file. To avoid the trouble of mounting your Google Drive, you can fetch the data file directly from this URL: https://s3.amazonaws.com/vargo.aprd6342/data/finalmaster-ratios.csv\n","\n","This is done for you below."]},{"cell_type":"markdown","metadata":{"id":"3KA1efIWfUWh"},"source":["### Imports"]},{"cell_type":"code","metadata":{"id":"ha2md0chfTHp","executionInfo":{"status":"ok","timestamp":1649958411336,"user_tz":360,"elapsed":1059,"user":{"displayName":"Krista Labovich","userId":"02513835265626180771"}}},"source":["import pandas as pd\n","import pandas\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LassoLarsCV\n","import matplotlib.pyplot as plt"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oGlve50O6aHX"},"source":["### Load the data"]},{"cell_type":"markdown","source":["Be sure to upload the data file to the root of your Google Drive account and to mount your Google Drive in this notebook.\n","\n","For compatibility with the grader, do not change the path of the file."],"metadata":{"id":"0dVspaiLplai"}},{"cell_type":"code","metadata":{"id":"w0JH-BbkgtnO","colab":{"base_uri":"https://localhost:8080/","height":314},"executionInfo":{"status":"ok","timestamp":1649958452380,"user_tz":360,"elapsed":299,"user":{"displayName":"Krista Labovich","userId":"02513835265626180771"}},"outputId":"7f205fc8-498d-4d28-c806-89df4a3d4468"},"source":["alldata = pd.read_csv(\"drive/MyDrive/finalmaster-ratios.csv\")\n","alldata.head()"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   # Purchases  B01001001   B01001002  B01001003  B01001004  B01001005  \\\n","0           22     206252  469.226965  31.432422  35.219052  33.628765   \n","1            7      61399  486.538869  22.899396  21.531295  27.036271   \n","2            3      73170  489.859232  28.905289  36.271696  28.235616   \n","3           94     251724  505.585483  32.054949  31.757004  28.102207   \n","4            0      37382  495.586111  25.413301  29.318924  26.162324   \n","\n","   B01001006  B01001007  B01001008  B01001009  ...  B19001008  B19001009  \\\n","0  20.121017  12.610787   6.734480   6.225394  ...  49.409690  53.306757   \n","1  16.808091  28.355511  18.192479  13.534422  ...  59.231680  50.093078   \n","2  21.566216  12.218122   7.243406   7.380074  ...  63.996993  47.322923   \n","3  18.651380  12.080692   7.035483   7.686991  ...  54.790900  48.681562   \n","4  19.260607  12.893906   6.580707   7.062222  ...  58.883378  51.761414   \n","\n","   B19001010  B19001011   B19001012   B19001013  B19001014  B19001015  \\\n","0  42.318307  83.167229   89.249208  102.141470  52.872330  36.440765   \n","1  40.700626  92.612963  117.363344  113.344051  75.774243  33.000508   \n","2  42.505211  70.420610   90.033143   98.677692  54.703249  20.125056   \n","3  43.873381  84.717507  112.204444  127.137252  83.019904  43.731067   \n","4  47.310187  81.902582   93.793717  130.103014  71.982704  36.118530   \n","\n","   B19001016  B19001017  \n","0  23.446284  21.197485  \n","1  33.169741  24.792689  \n","2  11.890525  16.537397  \n","3  38.851729  40.427349  \n","4  31.603714  19.648989  \n","\n","[5 rows x 190 columns]"],"text/html":["\n","  <div id=\"df-47ffff8c-3278-4f3a-ad65-758416bdc501\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th># Purchases</th>\n","      <th>B01001001</th>\n","      <th>B01001002</th>\n","      <th>B01001003</th>\n","      <th>B01001004</th>\n","      <th>B01001005</th>\n","      <th>B01001006</th>\n","      <th>B01001007</th>\n","      <th>B01001008</th>\n","      <th>B01001009</th>\n","      <th>...</th>\n","      <th>B19001008</th>\n","      <th>B19001009</th>\n","      <th>B19001010</th>\n","      <th>B19001011</th>\n","      <th>B19001012</th>\n","      <th>B19001013</th>\n","      <th>B19001014</th>\n","      <th>B19001015</th>\n","      <th>B19001016</th>\n","      <th>B19001017</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>22</td>\n","      <td>206252</td>\n","      <td>469.226965</td>\n","      <td>31.432422</td>\n","      <td>35.219052</td>\n","      <td>33.628765</td>\n","      <td>20.121017</td>\n","      <td>12.610787</td>\n","      <td>6.734480</td>\n","      <td>6.225394</td>\n","      <td>...</td>\n","      <td>49.409690</td>\n","      <td>53.306757</td>\n","      <td>42.318307</td>\n","      <td>83.167229</td>\n","      <td>89.249208</td>\n","      <td>102.141470</td>\n","      <td>52.872330</td>\n","      <td>36.440765</td>\n","      <td>23.446284</td>\n","      <td>21.197485</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>7</td>\n","      <td>61399</td>\n","      <td>486.538869</td>\n","      <td>22.899396</td>\n","      <td>21.531295</td>\n","      <td>27.036271</td>\n","      <td>16.808091</td>\n","      <td>28.355511</td>\n","      <td>18.192479</td>\n","      <td>13.534422</td>\n","      <td>...</td>\n","      <td>59.231680</td>\n","      <td>50.093078</td>\n","      <td>40.700626</td>\n","      <td>92.612963</td>\n","      <td>117.363344</td>\n","      <td>113.344051</td>\n","      <td>75.774243</td>\n","      <td>33.000508</td>\n","      <td>33.169741</td>\n","      <td>24.792689</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>73170</td>\n","      <td>489.859232</td>\n","      <td>28.905289</td>\n","      <td>36.271696</td>\n","      <td>28.235616</td>\n","      <td>21.566216</td>\n","      <td>12.218122</td>\n","      <td>7.243406</td>\n","      <td>7.380074</td>\n","      <td>...</td>\n","      <td>63.996993</td>\n","      <td>47.322923</td>\n","      <td>42.505211</td>\n","      <td>70.420610</td>\n","      <td>90.033143</td>\n","      <td>98.677692</td>\n","      <td>54.703249</td>\n","      <td>20.125056</td>\n","      <td>11.890525</td>\n","      <td>16.537397</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>94</td>\n","      <td>251724</td>\n","      <td>505.585483</td>\n","      <td>32.054949</td>\n","      <td>31.757004</td>\n","      <td>28.102207</td>\n","      <td>18.651380</td>\n","      <td>12.080692</td>\n","      <td>7.035483</td>\n","      <td>7.686991</td>\n","      <td>...</td>\n","      <td>54.790900</td>\n","      <td>48.681562</td>\n","      <td>43.873381</td>\n","      <td>84.717507</td>\n","      <td>112.204444</td>\n","      <td>127.137252</td>\n","      <td>83.019904</td>\n","      <td>43.731067</td>\n","      <td>38.851729</td>\n","      <td>40.427349</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>37382</td>\n","      <td>495.586111</td>\n","      <td>25.413301</td>\n","      <td>29.318924</td>\n","      <td>26.162324</td>\n","      <td>19.260607</td>\n","      <td>12.893906</td>\n","      <td>6.580707</td>\n","      <td>7.062222</td>\n","      <td>...</td>\n","      <td>58.883378</td>\n","      <td>51.761414</td>\n","      <td>47.310187</td>\n","      <td>81.902582</td>\n","      <td>93.793717</td>\n","      <td>130.103014</td>\n","      <td>71.982704</td>\n","      <td>36.118530</td>\n","      <td>31.603714</td>\n","      <td>19.648989</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 190 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-47ffff8c-3278-4f3a-ad65-758416bdc501')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-47ffff8c-3278-4f3a-ad65-758416bdc501 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-47ffff8c-3278-4f3a-ad65-758416bdc501');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g9Umh-KvL5qg","executionInfo":{"status":"ok","timestamp":1649958452086,"user_tz":360,"elapsed":21836,"user":{"displayName":"Krista Labovich","userId":"02513835265626180771"}},"outputId":"f94608b5-c185-44d7-8a2d-06aeafea8e1a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"p3Y12PTdf0G-"},"source":["There are 190 variables in this dataset:\n","\n"," * One outcome variable \"# Purchases\", and\n"," * 189 possible predictors.\n","\n","Create a list of all the predictors you're going to feed into the LassoLarsCV model. Assuming you've loaded your data into a pandas dataframe and named that dataframe alldata, you can get a list of your variables easily:"]},{"cell_type":"code","metadata":{"id":"Ag4Bht2jf1gM","executionInfo":{"status":"ok","timestamp":1649958456972,"user_tz":360,"elapsed":121,"user":{"displayName":"Krista Labovich","userId":"02513835265626180771"}}},"source":["all_variables = list(alldata.columns.values)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fgEsd46Cg-Z6"},"source":["For the sake of this homework, we know that the first 8 variables aren't valid predictors of what we're interested in. The first variable in the list is the outcome variable, and the next 7 are repetitive of other variables, so we need to exclude them. Go ahead and remove those items from the list."]},{"cell_type":"code","metadata":{"id":"AVxH3ZJ7h2XM","executionInfo":{"status":"ok","timestamp":1649958458375,"user_tz":360,"elapsed":349,"user":{"displayName":"Krista Labovich","userId":"02513835265626180771"}}},"source":["all_variables = all_variables[8:]"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"gFnLM_j8h7g4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649958459659,"user_tz":360,"elapsed":130,"user":{"displayName":"Krista Labovich","userId":"02513835265626180771"}},"outputId":"36c6e472-cc11-4f5e-8ab6-62f91dc2c79c"},"source":["all_variables[0:5]"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['B01001008', 'B01001009', 'B01001010', 'B01001011', 'B01001012']"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"rWgBiKa-h-B2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649958462240,"user_tz":360,"elapsed":5,"user":{"displayName":"Krista Labovich","userId":"02513835265626180771"}},"outputId":"b962fce9-0bc3-4941-cfb8-3b93be2070fe"},"source":["len(all_variables)"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["182"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"kx1tcJKmlO6f"},"source":["Now, let's get our columns ready in pandas. Your target variable should be:\n"]},{"cell_type":"code","metadata":{"id":"2x1-ozKSlevy","executionInfo":{"status":"ok","timestamp":1649958464144,"user_tz":360,"elapsed":344,"user":{"displayName":"Krista Labovich","userId":"02513835265626180771"}}},"source":["#load predictors into dataframe\n","predictors = alldata[all_variables]"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"mkTX9hQbiB3d","executionInfo":{"status":"ok","timestamp":1649958465463,"user_tz":360,"elapsed":3,"user":{"displayName":"Krista Labovich","userId":"02513835265626180771"}}},"source":["#load target into dataframe\n","target = alldata['# Purchases']   "],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KgjJMLwW7Xdj"},"source":["### Split the data into train and test sets"]},{"cell_type":"markdown","metadata":{"id":"AfUoui08lvYW"},"source":["Remember, you have 182 predictors and one thing you're predicting.\n","\n","Next, with most advanced regression models, we need to split the data into training and test sets. We train the model on the training set, and we test the model with the test set, to see how well the model actually performs. All this line of code is really doing is splitting the data (by rows) where we use 70% of the data to train, and 30% of the data to test. This method selects the data randomly, as to avoid any biases:\n"]},{"cell_type":"code","metadata":{"id":"SFV9wfRoiKKy","executionInfo":{"status":"ok","timestamp":1649958466765,"user_tz":360,"elapsed":290,"user":{"displayName":"Krista Labovich","userId":"02513835265626180771"}}},"source":["# split data into train and test sets, with 30% retained for test\n","\n","pred_train, pred_test, tar_train, tar_test = train_test_split(predictors, target, test_size=.3, random_state=123) "],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"LwIj2prtiOS9","colab":{"base_uri":"https://localhost:8080/","height":297},"executionInfo":{"status":"ok","timestamp":1649958468149,"user_tz":360,"elapsed":164,"user":{"displayName":"Krista Labovich","userId":"02513835265626180771"}},"outputId":"9653f8f6-8d54-40cf-cbdd-4edd722b5f79"},"source":["pred_train.head()"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     B01001008  B01001009  B01001010  B01001011  B01001012  B01001013  \\\n","309   9.023455   9.727580  25.827791  35.673712  35.082010  32.744787   \n","414   3.893270   7.439369  17.829688  28.740763  29.658285  25.715419   \n","691   4.916925   6.897677  18.199613  32.996994  30.293850  30.060821   \n","669  13.435004  11.656704  25.343091  29.644526  28.192094  26.004134   \n","534   8.665299  10.808659  32.425978  40.800392  33.451728  26.133684   \n","\n","     B01001014  B01001015  B01001016  B01001017  ...  B19001008  B19001009  \\\n","309  30.756668  36.123405  37.874843  34.283212  ...  50.537432  50.537432   \n","414  30.327828  29.261519  34.121906  28.492784  ...  50.298223  47.654184   \n","691  29.781185  33.602871  36.306014  31.808543  ...  57.632303  73.600489   \n","669  32.353872  30.966613  31.339031  32.009385  ...  47.653262  48.649901   \n","534  29.838636  26.179614  33.359870  25.858110  ...  62.869697  50.870373   \n","\n","     B19001010  B19001011   B19001012   B19001013  B19001014  B19001015  \\\n","309  47.085104  98.159242  117.770783  133.103178  79.693642  38.381758   \n","414  48.576523  73.541167   85.285618   71.757978  49.006948  15.126360   \n","691  35.668400  84.307128  133.374121  142.918324  90.302845  35.668400   \n","669  40.120524  79.870205   94.912504   83.277321  49.994206  17.406420   \n","534  49.349332  79.558898  103.684299  117.415920  41.786378  25.266182   \n","\n","     B19001016  B19001017  \n","309  29.547861  27.807192  \n","414   9.407858  19.307631  \n","691  31.141022  17.803610  \n","669  18.912968  17.452776  \n","534  16.984959  12.295082  \n","\n","[5 rows x 182 columns]"],"text/html":["\n","  <div id=\"df-722953ff-1cb1-47fa-a744-97d667d3d5a1\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>B01001008</th>\n","      <th>B01001009</th>\n","      <th>B01001010</th>\n","      <th>B01001011</th>\n","      <th>B01001012</th>\n","      <th>B01001013</th>\n","      <th>B01001014</th>\n","      <th>B01001015</th>\n","      <th>B01001016</th>\n","      <th>B01001017</th>\n","      <th>...</th>\n","      <th>B19001008</th>\n","      <th>B19001009</th>\n","      <th>B19001010</th>\n","      <th>B19001011</th>\n","      <th>B19001012</th>\n","      <th>B19001013</th>\n","      <th>B19001014</th>\n","      <th>B19001015</th>\n","      <th>B19001016</th>\n","      <th>B19001017</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>309</th>\n","      <td>9.023455</td>\n","      <td>9.727580</td>\n","      <td>25.827791</td>\n","      <td>35.673712</td>\n","      <td>35.082010</td>\n","      <td>32.744787</td>\n","      <td>30.756668</td>\n","      <td>36.123405</td>\n","      <td>37.874843</td>\n","      <td>34.283212</td>\n","      <td>...</td>\n","      <td>50.537432</td>\n","      <td>50.537432</td>\n","      <td>47.085104</td>\n","      <td>98.159242</td>\n","      <td>117.770783</td>\n","      <td>133.103178</td>\n","      <td>79.693642</td>\n","      <td>38.381758</td>\n","      <td>29.547861</td>\n","      <td>27.807192</td>\n","    </tr>\n","    <tr>\n","      <th>414</th>\n","      <td>3.893270</td>\n","      <td>7.439369</td>\n","      <td>17.829688</td>\n","      <td>28.740763</td>\n","      <td>29.658285</td>\n","      <td>25.715419</td>\n","      <td>30.327828</td>\n","      <td>29.261519</td>\n","      <td>34.121906</td>\n","      <td>28.492784</td>\n","      <td>...</td>\n","      <td>50.298223</td>\n","      <td>47.654184</td>\n","      <td>48.576523</td>\n","      <td>73.541167</td>\n","      <td>85.285618</td>\n","      <td>71.757978</td>\n","      <td>49.006948</td>\n","      <td>15.126360</td>\n","      <td>9.407858</td>\n","      <td>19.307631</td>\n","    </tr>\n","    <tr>\n","      <th>691</th>\n","      <td>4.916925</td>\n","      <td>6.897677</td>\n","      <td>18.199613</td>\n","      <td>32.996994</td>\n","      <td>30.293850</td>\n","      <td>30.060821</td>\n","      <td>29.781185</td>\n","      <td>33.602871</td>\n","      <td>36.306014</td>\n","      <td>31.808543</td>\n","      <td>...</td>\n","      <td>57.632303</td>\n","      <td>73.600489</td>\n","      <td>35.668400</td>\n","      <td>84.307128</td>\n","      <td>133.374121</td>\n","      <td>142.918324</td>\n","      <td>90.302845</td>\n","      <td>35.668400</td>\n","      <td>31.141022</td>\n","      <td>17.803610</td>\n","    </tr>\n","    <tr>\n","      <th>669</th>\n","      <td>13.435004</td>\n","      <td>11.656704</td>\n","      <td>25.343091</td>\n","      <td>29.644526</td>\n","      <td>28.192094</td>\n","      <td>26.004134</td>\n","      <td>32.353872</td>\n","      <td>30.966613</td>\n","      <td>31.339031</td>\n","      <td>32.009385</td>\n","      <td>...</td>\n","      <td>47.653262</td>\n","      <td>48.649901</td>\n","      <td>40.120524</td>\n","      <td>79.870205</td>\n","      <td>94.912504</td>\n","      <td>83.277321</td>\n","      <td>49.994206</td>\n","      <td>17.406420</td>\n","      <td>18.912968</td>\n","      <td>17.452776</td>\n","    </tr>\n","    <tr>\n","      <th>534</th>\n","      <td>8.665299</td>\n","      <td>10.808659</td>\n","      <td>32.425978</td>\n","      <td>40.800392</td>\n","      <td>33.451728</td>\n","      <td>26.133684</td>\n","      <td>29.838636</td>\n","      <td>26.179614</td>\n","      <td>33.359870</td>\n","      <td>25.858110</td>\n","      <td>...</td>\n","      <td>62.869697</td>\n","      <td>50.870373</td>\n","      <td>49.349332</td>\n","      <td>79.558898</td>\n","      <td>103.684299</td>\n","      <td>117.415920</td>\n","      <td>41.786378</td>\n","      <td>25.266182</td>\n","      <td>16.984959</td>\n","      <td>12.295082</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 182 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-722953ff-1cb1-47fa-a744-97d667d3d5a1')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-722953ff-1cb1-47fa-a744-97d667d3d5a1 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-722953ff-1cb1-47fa-a744-97d667d3d5a1');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"-OYFQIFemhVm"},"source":["We now have four new pandas data frames:\n","\n"," 1. pred_train, the predictors training set \n"," 2. pred_test, the predictors test test\n"," 3. target_train, the target training set\n"," 4. tar_test, the target test set\n"," \n","We'll feed these datasets to our model in the next step."]},{"cell_type":"markdown","metadata":{"id":"PaXJMHb-8ISY"},"source":["## Section 2\n","\n","You should now have the data loaded as described above."]},{"cell_type":"markdown","metadata":{"id":"QO-kOxWi9vTx"},"source":["### Build the model"]},{"cell_type":"markdown","metadata":{"id":"D4dXrlaF8vsu"},"source":["Build a [LassoLarsCV](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsCV.html) model with the following parameters:\n","\n","  * cv = 10 (this performs a 10-fold cross validation, to make sure our results aren't do to random ordering of the data)\n","  * precompute=False (precompute is not necessary)"]},{"cell_type":"code","metadata":{"id":"cFlYjGCfiSt4","executionInfo":{"status":"ok","timestamp":1649958474783,"user_tz":360,"elapsed":156,"user":{"displayName":"Krista Labovich","userId":"02513835265626180771"}}},"source":["model = LassoLarsCV(cv=10, precompute=False)"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eDGmYQLq9-7l"},"source":["Then fit the newly created model with the training data.\n","\n","To do this, [use .fit() on your newly created model object](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsCV.html#sklearn.linear_model.LassoLarsCV.fit). Remember what X and y mean from doing regression?"]},{"cell_type":"markdown","source":["Note: The following code cell produces a large number of Convergence Warnings which may be safely ignored for the purposes  of this assignment."],"metadata":{"id":"-ceJEa1ssSrl"}},{"cell_type":"code","metadata":{"id":"mA1w21Ya-SN8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649958704157,"user_tz":360,"elapsed":635,"user":{"displayName":"Krista Labovich","userId":"02513835265626180771"}},"outputId":"b01a200e-4185-46de-da63-32db389481dd"},"source":["model.fit(pred_train, tar_train)"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), LassoLarsCV())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.496e+00, with an active set of 5 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.098e+00, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=7.329e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.051e-01, with an active set of 11 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 17 iterations, alpha=5.739e-01, previous alpha=5.739e-01, with an active set of 14 regressors.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.100e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=2.867e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=2.050e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=2.013e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=1.986e-01, previous alpha=1.960e-01, with an active set of 37 regressors.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.365e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.008e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.144e-01, with an active set of 9 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.642e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.644e-01, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.644e-01, with an active set of 13 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 16 iterations, alpha=5.626e-01, previous alpha=5.354e-01, with an active set of 13 regressors.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=7.312e-01, with an active set of 9 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=6.185e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=3.656e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=3.656e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.592e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.520e-01, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=2.304e-01, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 39 iterations, alpha=2.027e-01, previous alpha=1.989e-01, with an active set of 34 regressors.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.916e-01, with an active set of 8 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=3.645e-01, with an active set of 17 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=3.456e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=1.867e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=1.734e-01, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=1.709e-01, with an active set of 40 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=1.692e-01, with an active set of 41 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 56 iterations, alpha=1.668e-01, previous alpha=1.607e-01, with an active set of 45 regressors.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.572e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.132e+00, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.640e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=3.863e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=3.863e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.560e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=1.932e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=1.737e-01, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 64 iterations, alpha=1.702e-01, previous alpha=1.687e-01, with an active set of 45 regressors.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.644e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.178e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.316e-01, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=8.155e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=3.203e-01, previous alpha=3.135e-01, with an active set of 25 regressors.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=2.100e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.080e-01, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=8.721e-02, with an active set of 49 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=8.721e-02, with an active set of 49 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 61 iterations, alpha=7.637e-02, previous alpha=7.570e-02, with an active set of 56 regressors.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.152e-01, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.576e-01, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=1.383e-01, with an active set of 42 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=6.747e-02, with an active set of 70 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=6.747e-02, with an active set of 70 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 97 iterations, i.e. alpha=6.182e-02, with an active set of 73 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 105 iterations, alpha=5.834e-02, previous alpha=5.366e-02, with an active set of 80 regressors.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=7.586e-02, with an active set of 69 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=7.586e-02, with an active set of 69 regressors, and the smallest cholesky pivot element being 8.878e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 80 iterations, alpha=7.564e-02, previous alpha=7.319e-02, with an active set of 69 regressors.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.372e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=9.825e-01, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.732e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=5.285e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:688: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 16 iterations, alpha=5.186e-01, previous alpha=5.185e-01, with an active set of 15 regressors.\n","  ConvergenceWarning,\n"]},{"output_type":"execute_result","data":{"text/plain":["LassoLarsCV(cv=10, precompute=False)"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"D6PVt6TO9sOV"},"source":["Congrats. You've just built your first LASSO model. Next, let's see what variables were significant."]},{"cell_type":"markdown","metadata":{"id":"yb2VFy7t_H_W"},"source":["### Build coefficient chart\n","\n","Next, let's extract the coefficients from the model."]},{"cell_type":"code","metadata":{"id":"9_A7diGuG_7I","colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"status":"ok","timestamp":1649958732814,"user_tz":360,"elapsed":141,"user":{"displayName":"Krista Labovich","userId":"02513835265626180771"}},"outputId":"bd791980-1b74-4669-9055-f163c798ac37"},"source":["pd.options.display.float_format = '{:.5f}'.format\n","predictors_model = pd.DataFrame(all_variables)\n","predictors_model.columns = ['label']\n","predictors_model['coeff'] = model.coef_\n","predictors_model"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["         label   coeff\n","0    B01001008 0.00000\n","1    B01001009 0.00000\n","2    B01001010 0.00000\n","3    B01001011 0.00000\n","4    B01001012 0.00000\n","..         ...     ...\n","177  B19001013 0.00000\n","178  B19001014 0.00000\n","179  B19001015 0.00000\n","180  B19001016 0.00000\n","181  B19001017 1.48344\n","\n","[182 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-4efe88fa-cebf-4cff-9833-2c044573f4e1\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>coeff</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>B01001008</td>\n","      <td>0.00000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>B01001009</td>\n","      <td>0.00000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>B01001010</td>\n","      <td>0.00000</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>B01001011</td>\n","      <td>0.00000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>B01001012</td>\n","      <td>0.00000</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>177</th>\n","      <td>B19001013</td>\n","      <td>0.00000</td>\n","    </tr>\n","    <tr>\n","      <th>178</th>\n","      <td>B19001014</td>\n","      <td>0.00000</td>\n","    </tr>\n","    <tr>\n","      <th>179</th>\n","      <td>B19001015</td>\n","      <td>0.00000</td>\n","    </tr>\n","    <tr>\n","      <th>180</th>\n","      <td>B19001016</td>\n","      <td>0.00000</td>\n","    </tr>\n","    <tr>\n","      <th>181</th>\n","      <td>B19001017</td>\n","      <td>1.48344</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>182 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4efe88fa-cebf-4cff-9833-2c044573f4e1')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-4efe88fa-cebf-4cff-9833-2c044573f4e1 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-4efe88fa-cebf-4cff-9833-2c044573f4e1');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"yc_x7uy6buye"},"source":["To build the coefficients chart of final predictors, filter the predictors for coefficient values greater than 0. This is done for you here:"]},{"cell_type":"code","metadata":{"id":"1wDmYPEQHFCZ","colab":{"base_uri":"https://localhost:8080/","height":328},"executionInfo":{"status":"ok","timestamp":1649958735811,"user_tz":360,"elapsed":132,"user":{"displayName":"Krista Labovich","userId":"02513835265626180771"}},"outputId":"0421a193-8922-4c89-874c-285e594b5237"},"source":["final_predictors = predictors_model[predictors_model['coeff'] > 0]\n","final_predictors"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["         label           coeff\n","6    B01001014         0.85586\n","28   B01001036         2.50535\n","29   B01001037         0.88929\n","30   B01001038         1.53163\n","46   B02001005         0.41253\n","96   B13014026         0.48004\n","97   B13014027         0.69786\n","113  B13016001 875104587.39904\n","181  B19001017         1.48344"],"text/html":["\n","  <div id=\"df-29d0f202-46e9-45f6-a9c5-791dfaed3d71\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>coeff</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>6</th>\n","      <td>B01001014</td>\n","      <td>0.85586</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>B01001036</td>\n","      <td>2.50535</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>B01001037</td>\n","      <td>0.88929</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>B01001038</td>\n","      <td>1.53163</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>B02001005</td>\n","      <td>0.41253</td>\n","    </tr>\n","    <tr>\n","      <th>96</th>\n","      <td>B13014026</td>\n","      <td>0.48004</td>\n","    </tr>\n","    <tr>\n","      <th>97</th>\n","      <td>B13014027</td>\n","      <td>0.69786</td>\n","    </tr>\n","    <tr>\n","      <th>113</th>\n","      <td>B13016001</td>\n","      <td>875104587.39904</td>\n","    </tr>\n","    <tr>\n","      <th>181</th>\n","      <td>B19001017</td>\n","      <td>1.48344</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-29d0f202-46e9-45f6-a9c5-791dfaed3d71')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-29d0f202-46e9-45f6-a9c5-791dfaed3d71 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-29d0f202-46e9-45f6-a9c5-791dfaed3d71');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"h9PcL4SYQxNL"},"source":["The variable final_predictors should now be a DataFrame of the 9 labels with coefficients greater than zero."]},{"cell_type":"markdown","metadata":{"id":"n4KUJuvZXBDu"},"source":["## Section 3"]},{"cell_type":"markdown","metadata":{"id":"_GEZVBsoAzGb"},"source":["Now we know which variables most predicted sales. Because of the aforementioned feature section feature of LASSO, any coefficients that are non-zero are significant, and thus variables that truly predict unique amounts of sales. The higher the value of the coefficient, the steeper the relationship between sales and that variable."]},{"cell_type":"markdown","metadata":{"id":"Ku-ROuUnBMUH"},"source":["### Finding meaning in the data\n","\n","Let's figure out **what these Census variables actually mean**. To do this, we're going to perform a simple hack, and have you Google the variable name + \"Census\", [here's an example](https://www.google.com/search?q=census+B01001036&oq=census+B01001036&aqs=chrome..69i57.4071j1j4&sourceid=chrome&ie=UTF-8).  Click on the first result from socialexplorer.com. For this example, I'd report: **Females aged 30 to 34 Years**."]},{"cell_type":"markdown","metadata":{"id":"Dhlj4FZQCOzi"},"source":["To record your findings, create a dictionary of keys that are named according to the census codes. Assign each key the string value of the description of that code. Do this for all of the codes in the coefficient chart that displayed above. The code from the Google search example above has been completed for you as an example:"]},{"cell_type":"code","metadata":{"id":"jVEGvrMcC4yY","executionInfo":{"status":"ok","timestamp":1649958855407,"user_tz":360,"elapsed":336,"user":{"displayName":"Krista Labovich","userId":"02513835265626180771"}}},"source":["descriptors = {\n","    'B01001036': 'Females aged 30 to 34 years',\n","    # Complete the remaining assignments from the codes in the positive\n","    # coefficients chart. Be sure to name the keys exactly the same as the codes.\n","    'B01001014': 'Males aged 40 to 44 years',\n","    'B01001037': 'Females aged 35 to 39 years',\n","    'B01001038': 'Females aged 40 to 44 years',\n","    'B02001005': 'Asian',\n","    'B13014026': 'Females with a bachelors degree aged 15 to 50 years who had a birth in the past 12 months',\n","    'B13014027': 'Females with a graduate/professional degree aged 15 to 50 years who had a birth in the past 12 months',\n","    'B13016001': 'Females aged 15 to 50 years who had a birth in the past 12 months',\n","    'B19001017': 'Households with over $200000 household income'\n","}"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lrI7RPWwSnW2"},"source":["### Choosing the top predictors"]},{"cell_type":"markdown","metadata":{"id":"NEzFBdc3EUDE"},"source":["If I had to report only two census variables to my boss that most steeply predicted sales, what would those be? To answer this question, create a dataframe called top_predictors which contains only the top 2 rows of final_predictors by coefficient.\n","\n","Pandas [sort_values](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html) will be useful here.\n"]},{"cell_type":"code","metadata":{"id":"FOYEqEtZJAZD","colab":{"base_uri":"https://localhost:8080/","height":152},"executionInfo":{"status":"ok","timestamp":1649958988572,"user_tz":360,"elapsed":295,"user":{"displayName":"Krista Labovich","userId":"02513835265626180771"}},"outputId":"0715fb97-fe87-4f48-9323-cf7716e64a50"},"source":["# Assign top_predictors to a frame of the top 2 labels. The easiest way to do\n","# this is probably to reverse sort final_predictors by coeff and take the [:2]\n","# slice of the results\n","\n","# complete this by setting top_predictors:\n","\n","#sorting values by the coeff value, then sorting it in descending order, then selecting the top two\n","top_predictors = final_predictors.sort_values(by=['coeff'], ascending = False)[0:2]\n","\n","top_predictors\n"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["         label           coeff\n","113  B13016001 875104587.39904\n","28   B01001036         2.50535"],"text/html":["\n","  <div id=\"df-6b8f1b01-bd29-456c-81a0-54aa630897ad\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>coeff</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>113</th>\n","      <td>B13016001</td>\n","      <td>875104587.39904</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>B01001036</td>\n","      <td>2.50535</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6b8f1b01-bd29-456c-81a0-54aa630897ad')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-6b8f1b01-bd29-456c-81a0-54aa630897ad button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-6b8f1b01-bd29-456c-81a0-54aa630897ad');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"MJ15m_oOMUCY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649958991621,"user_tz":360,"elapsed":946,"user":{"displayName":"Krista Labovich","userId":"02513835265626180771"}},"outputId":"2c7b31cb-0a86-4bc9-9536-11c0536bb562"},"source":["#~~ grader-ignore:\n","top_predictors.label.values\n","#~~ /grader-ignore\n"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['B13016001', 'B01001036'], dtype=object)"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"8m438MIlQXXj"},"source":["If you did all of the above correctly, you should get a simple report below that you can copy-paste into an email to your boss:"]},{"cell_type":"code","metadata":{"id":"ZjAgnC2MJlny","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649958995354,"user_tz":360,"elapsed":113,"user":{"displayName":"Krista Labovich","userId":"02513835265626180771"}},"outputId":"d54faeaf-b7f5-4ccf-f094-6b77316ef542"},"source":["#~~ grader-ignore:\n","print(f\"\"\"\n","In areas where there are { descriptors[top_predictors.label.values[0]] } and \n","{ descriptors[top_predictors.label.values[1]]}, we sell more Bobo Bars.\n","\"\"\")\n","#~~ /grader-ignore"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","In areas where there are Females aged 15 to 50 years who had a birth in the past 12 months and \n","Females aged 30 to 34 years, we sell more Bobo Bars.\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"zHXxECExEXtu"},"source":["### Mean squared error\n","\n","Next, lets take a look at the mean squared error for the training and training set:"]},{"cell_type":"code","metadata":{"id":"Hum2F_EhEas9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649959002266,"user_tz":360,"elapsed":125,"user":{"displayName":"Krista Labovich","userId":"02513835265626180771"}},"outputId":"9dbc04ae-994b-4743-8827-f209dbe11105"},"source":["from sklearn.metrics import mean_squared_error\n","\n","train_error = mean_squared_error(tar_train, model.predict(pred_train))\n","train_error"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["22025.457128540853"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"_Y-MKalCT9Jf"},"source":["Do the same thing as above, but for the test data."]},{"cell_type":"code","metadata":{"id":"JtGjpHVjFsHU","executionInfo":{"status":"ok","timestamp":1649959042790,"user_tz":360,"elapsed":274,"user":{"displayName":"Krista Labovich","userId":"02513835265626180771"}}},"source":["# create a variable called test_error with is the mean squared error of the test data\n","\n","# Complete this by setting test_error\n","\n","\n","test_error = mean_squared_error(tar_test, model.predict(pred_test))"],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nmKfM0kpGGI5"},"source":["Compare the test and training results above and consider the following questions: Are the training and text set mean squared errors similar? What does that mean practically? Think back to your stats class, or Google it!"]},{"cell_type":"markdown","metadata":{"id":"jrNYKDih8824"},"source":["### r-squared\n","\n","Next, let's see what our R-squared is for the training set: \n","\n"," \n"]},{"cell_type":"code","metadata":{"id":"vab6Cjv8GLrR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649959046481,"user_tz":360,"elapsed":114,"user":{"displayName":"Krista Labovich","userId":"02513835265626180771"}},"outputId":"fd1e9f9c-85be-4935-ef2b-9d9ff3905fe7"},"source":["rsquared_train = model.score(pred_train,tar_train)\n","rsquared_train"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.24002329299858283"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"vuGtgsj1GPYS","executionInfo":{"status":"ok","timestamp":1649959094001,"user_tz":360,"elapsed":553,"user":{"displayName":"Krista Labovich","userId":"02513835265626180771"}}},"source":["# create a variable called rsquared_test that is the r-square score for the test data\n","\n","# Complete this by setting rsquared_test\n","rsquared_test = model.score(pred_test, tar_test )"],"execution_count":26,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"verIc5C4GbCh"},"source":["Compare the r-squared results, and consider the following questions for thought and discusssion:"]},{"cell_type":"markdown","metadata":{"id":"V6SGZgOrGeDt"},"source":["If your boss asked, \"How well does Census data, overall, predict sales?\" What would you say? Why?"]},{"cell_type":"markdown","metadata":{"id":"zu2eSIxRGhtS"},"source":["### Find the y-intercept\n","\n","Finally, let's see what our y-intercept is, so we can interpret what our baseline sales number looks like, all things considered."]},{"cell_type":"code","metadata":{"id":"hNjDMU1IGqPq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649959100612,"user_tz":360,"elapsed":119,"user":{"displayName":"Krista Labovich","userId":"02513835265626180771"}},"outputId":"bdec2e92-9fc1-488a-e257-c3fb09035889"},"source":["y_intercept = model.intercept_\n","y_intercept"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["22.196684711205144"]},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"wXuURCymGsvN"},"source":["What is our baseline sales number? What does that mean,  practically? Think back to what y-intercepts mean in regression models."]}]}